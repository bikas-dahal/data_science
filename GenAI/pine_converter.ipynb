{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\break\\django\\nlp-env\\lib\\site-packages (0.1.20)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (0.1.57)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\break\\django\\nlp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\break\\django\\nlp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\break\\django\\nlp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\break\\django\\nlp-env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\break\\django\\nlp-env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\break\\django\\nlp-env\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\break\\django\\nlp-env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\break\\django\\nlp-env\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\break\\django\\nlp-env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\break\\django\\nlp-env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pinecone-client) (2024.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pinecone-client) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pinecone-client) (4.11.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pinecone-client) (2.2.1)\n",
      "Requirement already satisfied: colorama in d:\\break\\django\\nlp-env\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n",
      "Downloading pinecone_client-4.1.0-py3-none-any.whl (215 kB)\n",
      "   ---------------------------------------- 0.0/215.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/215.5 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 41.0/215.5 kB 495.5 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 112.6/215.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 194.6/215.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 215.5/215.5 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pinecone-client\n",
      "Successfully installed pinecone-client-4.1.0\n",
      "Requirement already satisfied: pypdf in d:\\break\\django\\nlp-env\\lib\\site-packages (3.17.4)\n",
      "Requirement already satisfied: openai in d:\\break\\django\\nlp-env\\lib\\site-packages (1.30.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in d:\\break\\django\\nlp-env\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\break\\django\\nlp-env\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\break\\django\\nlp-env\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\break\\django\\nlp-env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\break\\django\\nlp-env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\break\\django\\nlp-env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\break\\django\\nlp-env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\break\\django\\nlp-env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Requirement already satisfied: colorama in d:\\break\\django\\nlp-env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: tiktoken in d:\\break\\django\\nlp-env\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\break\\django\\nlp-env\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\break\\django\\nlp-env\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\break\\django\\nlp-env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\break\\django\\nlp-env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\break\\django\\nlp-env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\break\\django\\nlp-env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install pinecone-client\n",
    "!pip install pypdf \n",
    "!pip install openai \n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "from langchain.llms import OpenAI \n",
    "from langchain.vectorstores import Pinecone \n",
    "from langchain.chains import RetrievalQA \n",
    "from langchain.prompts import PromptTemplate\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader('pdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='     \\nResNet-based approach for Detection and \\nClassification of Plant Leaf Diseases \\n \\nVinod Kumar \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nVinod_k@dtu.ac.in Hritik Arora \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nhritikarora_bt2k16@dtu.ac.in\\n \\nHarsh \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nhkr641@gmail.com \\n \\n \\nAbstract — Crop disease is a serious concern for safety of food, \\nbut its fast detection still remains difficult in different parts of the \\nworld because of the lack of proper infrastructure. Automatic \\nidentification of plant diseases is necessary for food security, yield \\nloss estimation and management of disease. With the worldwide \\nincrease in digital cameras and continuous improvement in \\ncomputer vision domain, the automated techniques for detection \\nof disease are highly in demands in precision agriculture, highly \\nproductive plant phenotype, smart greenhouse and much more. \\nWorking on an open dataset which includes 15200 images of crop \\nleaves, a Residual Network (ResNe t34) was trained to perform \\nthis task of classification. The proposed ResNet34 model \\naccomplished a 99.40% accuracy on a test set, illustrating the \\nviability of the proposed model. Overall, the process of training \\nResNet models on an open image dataset provides a sound way \\ntowards crop disease detection using automated networks on an \\nenormous global scale. \\n \\n    Keywords - Leaf disease detection & classification, Convolutional \\nNeural Netwo rk (CNN), Deep Learning (DL), Computer Vision, \\nResidual Network (ResNet). \\n \\nI. INTRODUCTION  \\n    Since the dawn of time, humankind has been dependent on \\ncrops for living, our forefathers used to cover long stretches \\nfinding food, nothing new in that the first human race began \\nafter the discovery of agriculture. Crops are an integral part of \\nour lives. It will be impossible for humans to survive without \\ncrops. Crop diseases spoil the agricultural yield. It poses a big \\nproblem to the safety of food. Therefore, detection and \\nclassification of crop diseases has a major role in guaranteeing \\na better yield, better quality & better productivity of edible \\ncrops. The traditional techniques of diagnosis of diseases \\ndemands plenty of field involvement & proficiency. Jatin Sisodia \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\njatinsisodia715@gmail.com  \\n \\n \\nPlant pathologies can be identified using various routines. Few \\nailments do not show any perceptible symptoms, or they take \\ntoo long to exhibit any noticeable symptoms and hence in \\nthese conditions, an advanced examination is required. \\nHowever, almost all ailments display some sort of reflection in \\nthe visible stretch of spectrum, therefore inspection through \\neyes by experienced professionals is the common approach \\nadopted in real time. But, providing a detailed report of crop \\ndisease requires that pathologists must be equipped with a \\nsuperior observation skill set in order to diagnose \\ncharacteristic traits variations shown by diseased crop plants \\n[1]. It is often difficult since incompete nt farmers and \\nhorticulturists face trouble diagnosing it as compared to a \\nprofessional pathologist and often produce inappropriate \\ndiagnosis. But nowadays, due to advancements in internet and \\ndigital technologies, farmers can utilize crop images and can \\ntake help from crop pathologists to evaluate crop diseases \\nremotely. Though in this case the evaluation is prone to less \\nefficiency and wrong judgments. \\n    Moreover, research shows that climate variations [2] can \\ninterfere in stages and rates of germ growth and this also \\nmutates host, which could pave the way for physiological \\nchanges [3]. The fact that nowadays, diseases are passed on \\nworldwide more easily further complicates the situation. Well \\ntimed and exact diagnosis of crop diseases, including early \\nsafeguard measures is the foundation of precision agronomics.   \\n    Automated networks for disease identification can  address \\nthese problems with sophisticated analysis . The recent \\nevolution in computer vision ma de digital cameras a very \\nuseful technology to identify and classify diseases. An \\nautomated set -up built to detect crop plant pathologies using \\ncrop’s visuals and visible symptoms will prove to be of great \\naid, not only to non -professional horticulturists but also to \\nexperienced professionals as a method of confirmation of \\ndisease identified and classified.  \\n Proceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)\\nIEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 495\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply. ', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ResNet-based approach for Detection and \\nClassification of Plant Leaf Diseases \\n \\nVinod Kumar \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nVinod_k@dtu.ac.in Hritik Arora \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nhritikarora_bt2k16@dtu.ac.in\\n \\nHarsh \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nhkr641@gmail.com', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='Abstract — Crop disease is a serious concern for safety of food, \\nbut its fast detection still remains difficult in different parts of the \\nworld because of the lack of proper infrastructure. Automatic \\nidentification of plant diseases is necessary for food security, yield \\nloss estimation and management of disease. With the worldwide \\nincrease in digital cameras and continuous improvement in \\ncomputer vision domain, the automated techniques for detection', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='of disease are highly in demands in precision agriculture, highly \\nproductive plant phenotype, smart greenhouse and much more. \\nWorking on an open dataset which includes 15200 images of crop \\nleaves, a Residual Network (ResNe t34) was trained to perform \\nthis task of classification. The proposed ResNet34 model \\naccomplished a 99.40% accuracy on a test set, illustrating the \\nviability of the proposed model. Overall, the process of training', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='ResNet models on an open image dataset provides a sound way \\ntowards crop disease detection using automated networks on an \\nenormous global scale. \\n \\n    Keywords - Leaf disease detection & classification, Convolutional \\nNeural Netwo rk (CNN), Deep Learning (DL), Computer Vision, \\nResidual Network (ResNet). \\n \\nI. INTRODUCTION  \\n    Since the dawn of time, humankind has been dependent on \\ncrops for living, our forefathers used to cover long stretches', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='finding food, nothing new in that the first human race began \\nafter the discovery of agriculture. Crops are an integral part of \\nour lives. It will be impossible for humans to survive without \\ncrops. Crop diseases spoil the agricultural yield. It poses a big \\nproblem to the safety of food. Therefore, detection and \\nclassification of crop diseases has a major role in guaranteeing \\na better yield, better quality & better productivity of edible', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='crops. The traditional techniques of diagnosis of diseases \\ndemands plenty of field involvement & proficiency. Jatin Sisodia \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\njatinsisodia715@gmail.com  \\n \\n \\nPlant pathologies can be identified using various routines. Few \\nailments do not show any perceptible symptoms, or they take \\ntoo long to exhibit any noticeable symptoms and hence in \\nthese conditions, an advanced examination is required.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='However, almost all ailments display some sort of reflection in \\nthe visible stretch of spectrum, therefore inspection through \\neyes by experienced professionals is the common approach \\nadopted in real time. But, providing a detailed report of crop \\ndisease requires that pathologists must be equipped with a \\nsuperior observation skill set in order to diagnose \\ncharacteristic traits variations shown by diseased crop plants \\n[1]. It is often difficult since incompete nt farmers and', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='horticulturists face trouble diagnosing it as compared to a \\nprofessional pathologist and often produce inappropriate \\ndiagnosis. But nowadays, due to advancements in internet and \\ndigital technologies, farmers can utilize crop images and can \\ntake help from crop pathologists to evaluate crop diseases \\nremotely. Though in this case the evaluation is prone to less \\nefficiency and wrong judgments. \\n    Moreover, research shows that climate variations [2] can', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='interfere in stages and rates of germ growth and this also \\nmutates host, which could pave the way for physiological \\nchanges [3]. The fact that nowadays, diseases are passed on \\nworldwide more easily further complicates the situation. Well \\ntimed and exact diagnosis of crop diseases, including early \\nsafeguard measures is the foundation of precision agronomics.   \\n    Automated networks for disease identification can  address \\nthese problems with sophisticated analysis . The recent', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='evolution in computer vision ma de digital cameras a very \\nuseful technology to identify and classify diseases. An \\nautomated set -up built to detect crop plant pathologies using \\ncrop’s visuals and visible symptoms will prove to be of great \\naid, not only to non -professional horticulturists but also to \\nexperienced professionals as a method of confirmation of \\ndisease identified and classified.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='Proceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)\\nIEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 495\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='With the advent of computer vision, there is a chance to \\nimprove and enrich the practice of crop plant conservation. \\nMultiple different techniques such as digital image processing, \\nfor example color analysis and thresholding [4] are recently \\ndeployed for detection of crop disease among all, deep CNNs \\nare used quite often. \\n    Research work has been going on regarding detection of \\nplant disease that employs machine learning algorithms. Some', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='of them used traditional machine learning algorithms and \\nothers focused on deep learning models. A support vector \\nmachine (SVM) based approach used on the Vine plants was \\nproposed in [5] that accomplished 96.25% accuracy on an \\naverage. The authors in [6] proposed a decision tree for \\nclassification over tomato leaf images, giving an accuracy of \\n97.3%. Deep Learning models have been widely used, \\nespecially CNN, due to its high efficacy in image processing.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='A combination of K-NN and ANN was used to get an \\naccuracy of 94.67% as proposed in [7]. In [8], a CNN is \\ntrained using 800 cucumber leaf images, leading to a \\nclassification accuracy of 94.9%. [9] proposed a 13 layer-\\nCNN trained over a fruit image dataset using a momentum-\\nbased stochastic gradient descent as a learning algorithm. It \\nresulted in an accuracy of 94.94%. A novel method of \\nclassification was described in [10] that used a CNN with', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='Linear Vector Quantization (LVQ) algorithm with 500 images \\nof tomato leaves, giving rise to an accuracy of 86%. Pre-\\ntrained models have been found to perform better with even \\nless amount of data. A lot of methods that used pre-trained \\nmodels have been described in the literature. One of them is \\nmentioned in [11] where a comparison of AlexNet and \\nSqueezeNet has been done. AlexNet was found to have an \\naccuracy of 95.65% whereas SqueezNet had 94.3%.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='Numerous breakthroughs in image classification domain \\nhave been brought up by deep convolutional neural networks.  \\nThe network depth has its own importance and almost all \\npopular image classification methods utilize highly deep \\nmodels. In this paper, performance of Residual Network \\n(ResNet34) for plant diseases identification and classification \\nis discussed. The intuition of using Residual Networks is \\nmotivated from immense success in the sphere of computer', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='vision, for example image classification [12], [13] and well \\nknown task of object detection [14], [15] as shown by recent \\nstudies.  The idea of providing alternative connections to the \\nordinary connections and generating residual connections is \\nthe main motivation of working with ResNet. The application \\nof ResNet to the problem under consideration is not new as it \\nhas been explored in some past studies. Although this study \\nhas reported superhuman results, one of the main challenges', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='that is addressed in this study is that having so much similarity \\nbetween different leaf diseases can trick the model to make \\nincorrect predictions.      To check whether Residual Networks achieves better plant  \\ndisease classification results, ResNet34 [16], consisting of 34 \\nlayers  is employed . For the experiments,  ResNet 34 is \\nsuccessfully applied  on the dataset containing 15200 images.  \\nThis paper’s resear ch contributions are as follows :', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='• To detect diseases in plant leaf images.  \\n• To classify detected disease into various different classes.  \\n• To demonstrate the feasibility of using residual networks    \\n(ResNet) to classify plant diseases.  \\n• Learning the role of ResNets in improving scores of the \\ndisease ide ntification and classification.  \\n \\n    The rest of the paper is split into sections : Basic process of \\ndisease detection  and classification is covered under Section', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='II. CNN is described in Section III . Section IV provides details \\nregarding  the ResNet which is the  proposed model. Section V \\npresents the results of our experiments . The conclusion of the \\npaper is covered under Section VI.  Section VII gives possible \\nfuture work.  \\n \\nII. PROCESS OF PLANT DISEASE DETECTION & CLASSIFICATION  \\nIt comprises the following steps:  \\nA) Data Acquisition \\nB) Data Preprocessing \\nC) Feature Extraction \\nD) Classification', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='D) Classification \\n Fig 1 shows the flowchart of this process. \\n \\nFig. 1: Detection and classification process for leaf diseases \\n Proceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)\\nIEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 496\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 1}),\n",
       " Document(page_content='A. Data Acquisition \\n    The dataset consists of 15200 images that covers 14 crops \\nand is divided into 38 different classes. This data is derived \\nfrom the original Plant village dataset.  \\nB. Data Preprocessing \\n    It is done to convert the data in a form so that feature \\nextraction method and subsequent steps can work properly. \\nData augmentation and Normalization are done in this step.  \\n1. Data Augmentation : Data Augmentation is a popular', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='regularization method which provides a sound \\nsolution to the overfitting problem. It results in better \\nlearning of the model. Images are rotated by 90 \\ndegrees where the probability of whether an image is \\nrotated is 0.75.  \\n2. Data Normalization:  Normalization is done in order \\nto make all the pixel values have the same mean and \\nstandard deviation. This helps the model to learn \\nfaster. \\nC. Feature Extraction \\n    To solve the classification problem at hand, relevant', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='features are extracted first. Features in images are color, shape \\nand texture [17]. Systems that detect diseases using leaf \\nimages focus more on the texture feature. Some examples of \\nthe techniques that can be used are Grey-level co-occurrence \\nMatrix, auto-correlation, Gabor Transformation, 2D Gabor \\nfunction etc[17]. Gray-level co-occurrence matrix (GLCM) is \\na statistical method which characterizes the image texture by \\ncalculating how frequently pairs of pixels with certain values', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='and in a specified spatial relationship appear in an image. \\nAutocorrelation is a depiction of the degree of correlation \\nbetween a given time series and its previous version over \\nsuccessive time periods. The Gabor transform is a type of the \\nshort-time Fourier transform used to evaluate the phase \\ncontent and sinusoidal frequency of local sections of a signal \\nover time. Gabor functions can model the simple neurons \\nofthe brain’ visual cortex of mammals. 2 -D Gabor function is', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='used to simulate the space-based summation properties of \\nsimple cells (of the receptive field) in the visual cortex. \\nBesides high accuracies, extracting features automatically has \\nproved to be amongst the major advantages of using deep \\nlearning models. Since ResNet 34, which is a deep learning \\nmodel is used along with classification, it also handles \\nautomatic feature extraction. Thus, there is no need to use a \\nseparate feature extraction method in our proposed approach.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='D. Classification \\n    There are multiple choices available for classification. \\nSome of the classifiers that can be used in this step are : Logistic Regression, Radial Basis Function , Linear Vector \\nQuantization, ANN , Classification Trees, Support Vector \\nMachines, CNN, K- NN etc. [17]. ResNet 34 – a CNN \\narchitecture is used for classification purpose in the proposed \\nmethod. \\nIII. CONVOLUTIONAL NEURAL NETWORK  \\n    Inspiration from the way the human brain works, led to the', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='discovery of a class of algorithms, which falls in the sphere of \\ndeep learning. It involves the training of ANN to make \\npredictions. ANN are networks of neurons arranged in a multi-\\nlayer fashion. The output of one layer moves to the next layer \\nas input [10]. Features are automatically extracted by the deep \\nlearning models during training, therefore no separate method \\nis required for the feature extraction step of the basic process', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='that has been mentioned above [10]. The initial layers of ANN \\nlearn the low level features (like edges) and as deeper, higher \\nlevel features (like complete objects) are learned. \\n    CNN is a special type of ANN that is customized for image \\nprocessing. Researches have shown the capability of CNN to \\ngive high accuracy in image processing tasks. In conventional \\nANN, a single neuron in a layer takes its input from all \\nneurons in the previous layer. Thus, for image-based tasks, it', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='creates a problem of learning a large number of parameters. \\nCNN is preferred over ANN for image-related tasks due to \\nless number of parameters involved in comparison to ANN . \\nParameter sharing is responsible for this reduction. Same \\nparameters (filters in terminology of CNN) are used to get all \\nthe activations in the output volume from an input volume of \\nactivations. This is called parameter sharing. The goal of a \\nConvNet is to decrease the size of an image without losing', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='crucial features that helps in solving the  problem. Fig 2 shows \\nthe general architecture of CNN. A CNN architecture \\ncomprises four types of layers:  \\n \\n \\n \\n \\n \\n \\nA. Convolutional Layer \\n    CNN was named after the convolutional layer. The size of \\nthe image is reduced by applying a series of convolutional \\noperations. A filter/kernel is first placed on the upper left \\ncorner of the image and is then shifted along the width of the \\nimage towards right by some stride value. After covering the', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='entire width, the filter then hops down by the same stride \\nFig. 2: A general Architecture of CNN  \\n Proceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)\\nIEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 497\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 2}),\n",
       " Document(page_content='value and starts again from the left in order to cover the entire \\nwidth. This process is repeated till the whole image is \\ntraversed [10]. The sum of product of corresponding values in \\nthe overlapped portion of the image and filter is evaluated at a \\nsingle step. This gives rise to a new matrix (or volume) from \\nthe input matrix (or volume). Fig 3 shows convolution \\noperation applied to a 5 X 5 image with 3 X 3 filter in a \\nconvolutional layer.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='Fig. 3: Convolutional operator applied to 5 X 5 input and 3 X \\n3 filter. \\nB. Pooling Layer \\n    This layer has the functions of reducing the image size and \\nextracting the features that are dominant. A filter is placed and \\nmoved in the same manner as in case of a convolutional layer. \\nA function is applied at a single step in this layer. This \\nfunction can be a max function which finds the maximum of \\nall the values in the overlapped portion of the input image and', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='the filter (called Max-pooling) or it can be an average function \\n(called avg-pooling). Generally, filter size and stride used is 2. \\nMax-pooling is preferred over avg-pooling. Fig 4 shows max-\\npooling. \\n \\nFig. 4: Max-pooling (filter size 2 and stride 2) \\nC. Fully Connected Layer (FC) \\n    The output of a series of convolutional plus pooling layers \\nis a matrix( or volume) . This is flattened and then a sequence \\nof fully connected layers is used, similar to ANN. A single', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='neuron in a layer takes its input from all neurons in the \\nprevious layer . FC Layers are used only when the size of \\nimage is reduced enough by the series of convolutional plus \\npooling layers so that the fully connected layers don’t have \\nlarge no of parameters to learn. \\nD. Activation Layer     An activation function is applied to each element of the \\ninput matrix(or volume). So, the dimensions of input and \\noutput are the same for this layer. Linear activation functions', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='can only help in approximating linear hypothesis functions. \\nSince, a non-linear relationship generally occurs between \\ninput and ouput for complex problems, so nonlinear activation \\nfunctions are commonly used. ReLU function is mostly used \\nbecause it results in faster learning. \\nIV. PROPOSED MODEL : RESIDUAL NETWORKS  \\n    Past researches [18, 19] disclosed the utmost importance of \\nnetwork depth. Theoretically, the accuracy should increase by', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='stacking more and more layers in a neural network. In reality, \\nit turns out to be a misconception. By increasing the depth of \\nthe network, the accuracy tends to get saturated and then \\ndegrades quickly. This is known as the degradation problem. \\nSurprisingly, overfitting is not the cause. The phenomenon of \\nvanishing/exploding gradients [20,21]  in deep neural \\nnetworks leads to this notorious degradation problem. In the \\nvanishing gradient problem, the gradients become infinitely', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='small due to repeated multiplication during the \\nbackpropagation step, resulting in negligible updates to the \\nparameters. Exploding gradients is an issue in which gradients \\naccumulate and lead to very large updates to the parameters \\nduring training, preventing the model to learn from data. Prior \\nto the discovery of residual networks, this has been addressed \\nby the use of normalized initialization [ 21] and intermediate \\nnormalization layers.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='Residual network (ResNet) is a CNN architecture whose \\ncore building element is a residual block [16]. Fig 5 shows a \\nresidual block. \\n \\nFig. 5: A residual Block [16] \\nA residual block makes the use of skip connections to address \\nthe degradation problem. Connections that skip one or mor e \\nlayers are Shortcut connections (also known as skip \\nconnections). In the training process, the residual shortcut \\nguarantees the network integrity if the regular connection’s', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='coefficient converges to zero. The alternative connections \\nimproves the network by providing the option of choosing \\nthese shortcuts when required. In [16], it has been proposed \\nthat instead of the hypothesis function H(x) = F(x) + x, the \\nresidual function F(x) is learned by the layers. This is due to \\nthe fact that residual function is optimized easily. In order to Proceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='IEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 498\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 3}),\n",
       " Document(page_content='prove that a deeper network does not have higher training \\nerror than the shallow counterpart, the authors used a deep \\nnetwork that is made from the shallow network by just \\nconcatenating a residual block at the end [16] and then showed \\nthat the residual block acts as an identity mapping.    \\n    Fig 6 shows a ResNet34 architecture made from a plain- 34 \\nlayer convolution al neural network by just introducing skip \\nconnections. ResNet34 is a model that is pre-trained on', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 4}),\n",
       " Document(page_content='ImageNet Database. Pre-trained model helps in getting higher \\naccuracies by just using a small amount of data and it saves \\ntime. In our work, a pretrained ResNet34 model has been \\nemployed for plant disease detection. Deep neural networks \\ngive better results than the shallow ones, provided that the \\ndegradation problem is addressed using some suitable \\ntechnique. ResNet34 uses skip connections to solve this \\nproblem. The strength of ResNet34 to solve the degradation', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 4}),\n",
       " Document(page_content='problem to give higher accuracies and the advantages of this \\npre-trained model is the motivation of using it as the \\nclassification technique in our proposed work. \\n \\nV. EXPERIMENTAL RESULTS AND DISCUSSION  \\nA. Dataset  \\n    The proposed Residual Network model (ResNet34) is \\ntrained and tested using images from the ‘New Plant Diseases \\nDataset’, con taining a total of 15200 images, labelled with 38 \\ndifferent classes and covering 14 different crops . 80-20 rule i s', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 4}),\n",
       " Document(page_content='used for dividing the dataset, resulting in 12160 train images \\nand 3040 test images. \\nB. Training  \\n    Model is first trained using the train set images.  \\nClassification is then performed on the test set images with the \\ntrained model. The various parameters for the model are \\nsummarized in Table I. The variation of loss during training \\nand validation with respect to the number of batches processed \\nis illustrated in Figure 7.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 4}),\n",
       " Document(page_content='Fig. 6: Plain -34 layer CN N (left) and ResNet34 (right)[ 16] Proceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)\\nIEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 499\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 4}),\n",
       " Document(page_content='Table I \\nParameters of the trained ResNet34 model \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 7: Plot of Training and Validation Loss for the proposed \\nmode \\n \\nC. Performance Evaluation  \\n    Comparison of performance of ResNet with that of SVM, \\ndecision trees, logistic regression and K- NN is done on the \\nbasis of two metrics- accuracy and precision. The values for \\naccuracy and precision of other models were taken from [22]. \\nTwo classifier metrics- accuracy and precision are discussed', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 5}),\n",
       " Document(page_content='in the below subsections.  \\n  \\n    1. Accuracy:  The ratio of the number of correct predictions \\nto the total no of predictions made is known as accuracy .  \\n \\n \\n \\nTrue positive is the count of examples for which the actual \\nlabel is true and the model made a correct prediction. False \\nnegative is the count of examples for which the actual label is \\ntrue but the model made an incorrect prediction. True \\nNegative means the number of examples for wh ich the actual', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 5}),\n",
       " Document(page_content='label is false and the model made a correct prediction. False \\nPositive means the count of samples for which the actual label \\nis false but the model made an incorrect prediction.  \\n \\n \\n \\nThe accuracy of the proposed model comes out to be 99.40%. The results are shown in Table II. As can be seen clearly, for \\nmost of the classes, the accuracy comes out to be 100%. \\nComparison among accuracies of various models is shown in \\nFigure 8. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n        \\n  Parameters Value', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 5}),\n",
       " Document(page_content='Parameters Value \\nTotal parameters 21,832,038 \\nTraining set size 12,160 \\nTest set size 3,040 \\nLearning Rate 1e-6 – 1e-3 \\n        Fig. 8: Comparison of Avg Testing Accuracy of various models  \\n \\nProceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)\\nIEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 500', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 5}),\n",
       " Document(page_content='Authorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 5}),\n",
       " Document(page_content='2. Precision : Dividing the count of true p ositives by the total \\ncount of positive predictions made  by the model gives us the \\nprecision  of a class .  \\n \\nIn our work, the weighted average of the precision of each \\nclass is considered as the precision of the model. The weight \\nof a class is  computed as t he ratio of count  of examples of that \\nclass to the total count  of examples in the test set . The \\nprecision of the model comes out to be 0.9651. Comparison of', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='precisio n values between vari ous models is depicted in Fig 9 . \\n \\n \\nFig. 9 : Comparison of Precision of different models \\n \\nTable III shows the performance analysis of various models \\nbased on precision and accuracy. \\n \\nTable III  \\n Performance Analysis based on Accuracy and Precision \\n \\n \\n \\nVI. CONCLUSIONS  \\n    Deep learning techniques are being widely studied. This \\nwork showed that the Residual Network model (ResNet34) \\ncan accurately detect and classify disease from images of', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='leaves. The model is trained using the images from ‘New Plant \\nDiseases Dataset’, which contains images belonging to 38 \\ndifferent classes. The 80-20 rule is used for dividing the \\ndataset. An avg weighted precision of 96.51% and an accuracy \\nof 99.40% was achieved by our model. These two \\nperformance metrics for the ResNet model are also compared with that of four other techniques- SVM, K-NN, Decision \\nTree and Logistic Regression. The proposed model is found to', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='have higher accuracy and precision values compared to the \\nother four models, for which the values ranged between 50% \\nand 90%, demonstrating its superiority for the task of plant \\ndisease classification when compared with existing \\ntechniques.  \\n \\nVII. FUTUREWORK \\n    In future work, network models can be trained with the \\ndataset containing more diseases and crops. Dataset can be \\nincreased by adding more and more images as data points so', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='that the network can identify and classify a wider range of \\ndiseases and plant species. With the increased use of cameras \\nand improvement in their quality, it becomes more and more \\nlikely that accurate diagnoses using smartphones is only a \\nmatter of time. Also, models can be trained upon data such as \\npanorama view of land areas, aerial photos and also images of \\ndifferent stages of different diseases. Moreover, the effect of \\nimage rotation on the network could be explored.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='REFERENCES \\n \\n[1] M. B. Riley, M. R. Williamson, and O. Maloy, “Plant     disease \\ndiagnosis. The Plant Health Instructor,” 2002.  \\n[2] K. A. Garrett, S. P. Dendy, E. E. Frank, M. N. Rouse, and S. E. \\nTravers, “Climate change effects on plant disease: genomes to \\necosystems,” Annual Review of Phytopathology, vol. 44, pp. 489 – \\n509, 2006.  \\n[3] S. Chakraborty, A. V. Tiedemann, and P. S. Teng, “Climate \\nchange: potential impact  on plant diseases,” Environmental', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='Pollution, vol. 108, no. 3, pp. 317 –326, 2000.  \\n[4] J. G. Arnal Barbedo, “Digital image processing techniques for \\ndetecting, quantifying and classifying plant diseases,” \\nSpringerPlus, vol. 2, article 660, pp. 1 –12, 2013.  \\n[5] Pantazi X.E., Moshou D., Tamouridou A.A., Kasderidis S. (2016) \\nLeaf Disease Recognition in Vine Plants Based on Local Binary \\nPatterns and One Class Support Vector Machines. In: Iliadis L., \\nMaglogiannis I. (eds) Artificial Intelligence Applications and', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='Innovat ions. AIAI 2016. IFIP Advances in Information and \\nCommunication Technology, vol 475. Springer, Cham  \\n[6] H. Sabrol and K. Satish, \"Tomato plant disease classification in \\ndigital images using classification tree,\"  2016 International \\nConference on Communication a nd Signal Processing (ICCSP) , \\nMelmaruvathur, 2016, pp. 1242 -1246.  \\n[7] H. Al -Hiary, S. Bani -Ahmad, M. Reyalat, M. Braik, and Z. AL -\\nRahamneh, “Fast and accurate detection and classification of plant', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='diseases,” Machine Learning, vol. 14, p. 5, 2011.  \\n[8] Kawasaki, Yus uke & Uga, Hiroyuki & Kagiwada, Satoshi & \\nIyatomi, Hitoshi. (2015). Basic Study of Automated Diagnosis of \\nViral Plant Diseases Using Convolutional Neural Networks. 9475. \\n638-645. 10.1007/978 -3-319- 27863 -6_59.  \\n[9] Zhang Y -D, Dong Z, Chen X, Jia W, Du S, Muhamma d K, et al. \\nImage based fruit category classification by 13 -layer deep \\nconvolutional neural network and data augmentation. Multimed', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='Tools  Appl 2019;78(3):3613 –32. doi:10.1007/s11042 -017-5243 -3. \\n[10] M. Sardogan, A. Tuncer and Y. Ozen, \"Plant Leaf    Disease \\nDetection and Classification Based on CNN with LVQ \\nAlgorithm,\"  2018 3rd International Conference on Computer \\nScience and Engineering (UBMK) , Sarajevo, 2018, pp. 382 -385.  \\n[11] H. Durmus¸, E. O. Gunes ¨ ¸ and M. Kırcı, “Disease detection on \\nthe leaves of the tomato plants by using deep learning”, In Agro -Model Accuracy Precision \\nSVM  0.5069  0.5057', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='Decision Tree 0.7223 0.7202 \\nLogi stic Regression  0.8099  0.8088  \\nK-NN  0.8786 0.8806 \\nProposed Model  0.9940  0.9651  Proceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)\\nIEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 501\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 6}),\n",
       " Document(page_content='Geoinformatics, IEEE 6th International Conference on, pp. 1-5, \\n2017.  \\n[12] M. D. Zeiler and R. Fergus, “Visualizing and understanding \\nconvolutional networks,” in European Conference on Computer \\nVision. Springer, 2014, pp. 818–833.  \\n[13] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. \\nLeCun, “Overfeat: Integrated recognition, localization and \\ndetection using convolutional networks,” arXiv preprint \\narXiv:1312.6229, 2013.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 7}),\n",
       " Document(page_content='[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature \\nhierarchies for accurate object detection and semantic \\nsegmentation,” in Proceedings of the IEEE conference on \\ncomputer vision and pattern recognition, 2014, pp. 580 –587.  \\n[15] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r -cnn: Towards \\nreal-time object detection with region proposal networks,” in \\nAdvances in neural information processing systems, 2015, pp. 91 –\\n99.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 7}),\n",
       " Document(page_content='99. \\n[16] K. He, X. Zhang, S. Ren and J. Sun, \"Deep Residual Learning for \\nImage Recognition,\"  2016 IEEE Conference on Computer Vision \\nand Pattern Recognition (CVPR) , Las Vegas, NV, 2016, pp. 770 -\\n778 \\n[17] Gavhale, Kiran R. and Ujwalla Gawande. “An Overview of the \\nResearch on Plant Leaves Disease detection using Image \\nProcessing Techniques.”  IOSR Journal of Computer \\nEngineering  16 (2014): 10 -16. \\n[18] K. Simonyan and A. Zisserman. Very deep convolutional networks', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 7}),\n",
       " Document(page_content='for large -scale image recognition. In ICLR, 2015.  \\n[19] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. \\nErhan, V. Vanhoucke, and A. Rabinovich. Going deeper with \\nconvolutions. In CV PR, 2015  \\n[20] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term \\ndependencies with gradient descent is difficult. IEEE Transactions \\non Neural Networks, 5(2):157 –166, 1994 \\n[21] X. Glorot and Y. Bengio. Understanding the difficulty of training', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 7}),\n",
       " Document(page_content='deep feedforward neural networks. In AISTATS, 2010 \\n[22] G., Geetharamani & Pandian J, Arun. (2019). Identification of \\nplant leaf diseases using a nine -layer deep convolutional neural \\nnetwork. Computers & Electrical Engineering. 76. 323 -338. \\n10.1016/j.compeleceng.2019.04.011  \\n[23] Vijayakumar, T. (2019). COMPARATIVE STUDY OF \\nCAPSULE NEURAL NETWORK IN VARIOUS', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 7}),\n",
       " Document(page_content='APPLICATIONS. Journal of Artificial Intelligence, 1(01), 19 -27.   Proceedings of the International Conference on Electronics and Sustainable Communication Systems (ICESC 2020)\\nIEEE Xplore Part Number: CFP20V66-ART; ISBN: 978-1-7281-4108-4\\n978-1-7281-4108-4/20/$31.00 ©2020 IEEE 502\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 08,2020 at 03:12:25 UTC from IEEE Xplore.  Restrictions apply.', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 7})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = text_splitter.split_documents(data)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ResNet-based approach for Detection and \\nClassification of Plant Leaf Diseases \\n \\nVinod Kumar \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nVinod_k@dtu.ac.in Hritik Arora \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nhritikarora_bt2k16@dtu.ac.in\\n \\nHarsh \\nDepartment of Computer Engineering \\nDelhi Technological University, Delhi, India \\nhkr641@gmail.com', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0}),\n",
       " Document(page_content='Abstract — Crop disease is a serious concern for safety of food, \\nbut its fast detection still remains difficult in different parts of the \\nworld because of the lack of proper infrastructure. Automatic \\nidentification of plant diseases is necessary for food security, yield \\nloss estimation and management of disease. With the worldwide \\nincrease in digital cameras and continuous improvement in \\ncomputer vision domain, the automated techniques for detection', metadata={'source': 'pdfs\\\\[4].pdf', 'page': 0})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-based approach for Detection and \n",
      "Classification of Plant Leaf Diseases \n",
      " \n",
      "Vinod Kumar \n",
      "Department of Computer Engineering \n",
      "Delhi Technological University, Delhi, India \n",
      "Vinod_k@dtu.ac.in Hritik Arora \n",
      "Department of Computer Engineering \n",
      "Delhi Technological University, Delhi, India \n",
      "hritikarora_bt2k16@dtu.ac.in\n",
      " \n",
      "Harsh \n",
      "Department of Computer Engineering \n",
      "Delhi Technological University, Delhi, India \n",
      "hkr641@gmail.com\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract — Crop disease is a serious concern for safety of food, \n",
      "but its fast detection still remains difficult in different parts of the \n",
      "world because of the lack of proper infrastructure. Automatic \n",
      "identification of plant diseases is necessary for food security, yield \n",
      "loss estimation and management of disease. With the worldwide \n",
      "increase in digital cameras and continuous improvement in \n",
      "computer vision domain, the automated techniques for detection\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['OpenAI_API_KEY'] = 'sk-ADnvdnveEIFncncxnc2343nfd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-ADnvd****************3nfd. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m----> 2\u001b[0m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhatever\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\langchain_community\\embeddings\\openai.py:697\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m        Embedding for the text.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\langchain_community\\embeddings\\openai.py:668\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\langchain_community\\embeddings\\openai.py:494\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    492\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 494\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    500\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\langchain_community\\embeddings\\openai.py:116\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\openai\\resources\\embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-ADnvd****************3nfd. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings.embed_query('whatever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_ENV', '6417e512-b4b6-47e2-be05-41492c3c1589')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\break\\Django\\nlp-env\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPINECONE_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPINECONE_API_ENV\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\break\\Django\\nlp-env\\Lib\\site-packages\\pinecone\\deprecation_warnings.py:38\u001b[0m, in \u001b[0;36minit\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m    import os\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m    from pinecone import Pinecone, ServerlessSpec\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124m        )\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     31\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124minit is no longer a top-level attribute of the pinecone package.\u001b[39m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124mPlease create an instance of the Pinecone class instead.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n",
      "\u001b[1;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
     ]
    }
   ],
   "source": [
    "pinecone.init(\n",
    "    api_key = PINECONE_API_KEY,\n",
    "    environment = PINECONE_API_ENV\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Index.__init__() missing 1 required positional argument: 'host'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mpinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[1;31mTypeError\u001b[0m: Index.__init__() missing 1 required positional argument: 'host'"
     ]
    }
   ],
   "source": [
    "index_name = 'test'\n",
    "index = pinecone.Index(index_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m docsearch \u001b[38;5;241m=\u001b[39m Pinecone\u001b[38;5;241m.\u001b[39mfrom_texts([t\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text_chunks], \u001b[43membedding\u001b[49m, index_name\u001b[38;5;241m=\u001b[39mindex_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding' is not defined"
     ]
    }
   ],
   "source": [
    "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is resnet50' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docsearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mdocsearch\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(query, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docsearch' is not defined"
     ]
    }
   ],
   "source": [
    "docs = docsearch.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docsearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m'\u001b[39m, retriever \u001b[38;5;241m=\u001b[39m \u001b[43mdocsearch\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docsearch' is not defined"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm, chain_type='stuff', retriever = docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mqa\u001b[49m\u001b[38;5;241m.\u001b[39mrun(query)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa' is not defined"
     ]
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: chromadb\n",
      "Version: 0.4.24\n",
      "Summary: Chroma.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
      "License: \n",
      "Location: D:\\break\\Django\\nlp-env\\Lib\\site-packages\n",
      "Requires: bcrypt, build, chroma-hnswlib, fastapi, grpcio, importlib-resources, kubernetes, mmh3, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, opentelemetry-sdk, orjson, overrides, posthog, pulsar-client, pydantic, pypika, PyYAML, requests, tenacity, tokenizers, tqdm, typer, typing-extensions, uvicorn\n",
      "Required-by: crewai-tools, embedchain\n"
     ]
    }
   ],
   "source": [
    "!pip show chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
